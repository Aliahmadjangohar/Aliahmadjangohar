{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aliahmadjangohar/Aliahmadjangohar/blob/main/Object_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Set Up Google Colab**:\n",
        "\n",
        "Open Google Colab (https://colab.research.google.com/).\n",
        "Connect to a GPU if available. Go to \"Runtime\" > \"Change runtime type\" and select GPU."
      ],
      "metadata": {
        "id": "7Q7pWV2VLW9u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aZPJVVytZHG",
        "outputId": "d1040435-a24e-49fe-ba62-66cf2594f837"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Navigate to the Dataset Folder:**\n",
        "\n",
        "Use the cd command to navigate to the folder where your annotated dataset is stored."
      ],
      "metadata": {
        "id": "QT1Xf5ZvMQxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Directions\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDJKMRC73ULo",
        "outputId": "8aa1fb29-c683-41a0-83bc-afa125ff1b7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Directions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preprocessing:**\n",
        "\n",
        "Organize your dataset into folders named 'left', 'right', and 'straight'. Each of these folders should contain both the original images and their corresponding XML annotation files"
      ],
      "metadata": {
        "id": "y_7MgXXkMd0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "\n",
        "# Define a function to parse the XML annotation file\n",
        "def parse_annotation(annotation_path):\n",
        "    tree = ET.parse(annotation_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Extract the relevant information, e.g., 'class' (turn type)\n",
        "    for obj in root.findall('object'):\n",
        "        class_name = obj.find('name').text\n",
        "        return class_name  # Assuming only one object per image\n",
        "\n",
        "# Define a function to process original images and annotations\n",
        "def process_data(dataset_dir, output_dir):\n",
        "    for category in os.listdir(dataset_dir):\n",
        "        category_dir = os.path.join(dataset_dir, category)\n",
        "        output_category_dir = os.path.join(output_dir, category)\n",
        "\n",
        "        if not os.path.exists(output_category_dir):\n",
        "            os.makedirs(output_category_dir)\n",
        "\n",
        "        for filename in os.listdir(category_dir):\n",
        "            if filename.endswith('.jpeg'):\n",
        "                image_path = os.path.join(category_dir, filename)\n",
        "                annotation_path = os.path.splitext(image_path)[0] + \".xml\"\n",
        "\n",
        "                # Check if the corresponding XML annotation file exists\n",
        "                if os.path.exists(annotation_path):\n",
        "                    # Load the image\n",
        "                    image = cv2.imread(image_path)\n",
        "                    # Perform any necessary image processing here\n",
        "\n",
        "                    # Parse the annotation to get the label\n",
        "                    label = parse_annotation(annotation_path)\n",
        "\n",
        "                    # Save the processed image with the same filename\n",
        "                    processed_image_path = os.path.join(output_category_dir, filename)\n",
        "                    cv2.imwrite(processed_image_path, image)\n",
        "\n",
        "                    # You can also save the label or use it as needed\n",
        "                    # For example, you can save labels in a separate text file\n",
        "\n",
        "# Define the path to your dataset folder and the output folder\n",
        "dataset_dir = '/content/drive/MyDrive/Directions'\n",
        "output_dir = '/content/drive/MyDrive/dataset cricket'\n",
        "\n",
        "# Process the data\n",
        "process_data(dataset_dir, output_dir)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JYah42Lm4JoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Loading**:\n",
        "\n",
        "Load and preprocess your dataset. You can use libraries like OpenCV or PIL for image loading and parsing the XML files for annotations"
      ],
      "metadata": {
        "id": "u_CfPPeCN2GV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to parse the XML annotation file\n",
        "def parse_annotation(annotation_path):\n",
        "    tree = ET.parse(annotation_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Extract the relevant information, e.g., 'class' (turn type)\n",
        "    for obj in root.findall('object'):\n",
        "        class_name = obj.find('name').text\n",
        "        return class_name  # Assuming only one object per image\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z9RdiqFx5fD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "\n",
        "# Define a function to parse the XML annotation file\n",
        "def parse_annotation(annotation_path):\n",
        "    tree = ET.parse(annotation_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Initialize the class name variable\n",
        "    class_name = None\n",
        "\n",
        "    # Iterate through objects in the XML and find the class name\n",
        "    for obj in root.findall('object'):\n",
        "        class_name = obj.find('name').text\n",
        "        break  # Assuming only one object per image, so break after finding one\n",
        "\n",
        "    return class_name\n",
        "\n",
        "# Define a function to load and preprocess the dataset\n",
        "def load_and_preprocess_dataset(dataset_dir, image_extension='.jpg', annotation_extension='.xml'):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for filename in os.listdir(dataset_dir):\n",
        "        if filename.endswith(image_extension):\n",
        "            image_path = os.path.join(dataset_dir, filename)\n",
        "            annotation_filename = filename.replace(image_extension, annotation_extension)\n",
        "            annotation_path = os.path.join(dataset_dir, annotation_filename)\n",
        "\n",
        "            # Check if the corresponding XML annotation file exists\n",
        "            if os.path.exists(annotation_path):\n",
        "                # Load the image without resizing\n",
        "                image = cv2.imread(image_path)\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                # Parse the annotation to get the label\n",
        "                label = parse_annotation(annotation_path)\n",
        "\n",
        "                if label is not None:\n",
        "                    images.append(image)\n",
        "                    labels.append(label)\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Define the path to your dataset folder\n",
        "dataset_dir = '/content/drive/MyDrive/Directions'\n",
        "\n",
        "# Specify the file extensions\n",
        "image_extension = '.jpeg'\n",
        "annotation_extension = '.xml'\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "images, labels = load_and_preprocess_dataset(dataset_dir, image_extension, annotation_extension)\n",
        "\n",
        "# Check the shape of the loaded data\n",
        "print(\"Images shape:\", images.shape)\n",
        "print(\"Labels shape:\", labels.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSqDhmZj_aZU",
        "outputId": "81561495-3c3d-4f99-8486-3415b84390cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Images shape: (0,)\n",
            "Labels shape: (0,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Split the Dataset**:\n",
        "\n",
        "Split your dataset into training and validation sets. Typically, you'd use around 70-80% of the data for training and the rest for validation. You can use the train_test_split function from scikit-learn to achieve this."
      ],
      "metadata": {
        "id": "jCU27qcFP3Pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset into training and validation sets (e.g., 80% training, 20% validation)\n",
        "test_size = 0.2  # Adjust this percentage as needed\n",
        "random_state = 42  # For reproducibility\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=test_size, random_state=random_state)\n",
        "\n",
        "# Check the shapes of the training and validation sets\n",
        "print(\"Training images shape:\", X_train.shape)\n",
        "print(\"Training labels shape:\", y_train.shape)\n",
        "print(\"Validation images shape:\", X_val.shape)\n",
        "print(\"Validation labels shape:\", y_val.shape)\n"
      ],
      "metadata": {
        "id": "u4_ygdHbZ3fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Model Building**:\n",
        "\n",
        "Build a convolutional neural network (CNN) for image classification. You can use popular deep learning frameworks like TensorFlow or PyTorch. Define the architecture of your CNN model."
      ],
      "metadata": {
        "id": "KQImoSIWRPOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define the CNN model\n",
        "def create_cnn_model(input_shape, num_classes):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Convolutional Layer 1\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Convolutional Layer 2\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Convolutional Layer 3 (optional)\n",
        "    # You can add more convolutional layers as needed.\n",
        "\n",
        "    # Flatten the feature maps\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Fully Connected Layers\n",
        "    model.add(layers.Dense(128, activation='relu'))\n",
        "    model.add(layers.Dropout(0.5))  # Optional dropout layer for regularization\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))  # Output layer\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define the input shape and number of classes\n",
        "input_shape = (your_image_height, your_image_width, your_num_channels)  # Replace with actual values\n",
        "num_classes = your_num_classes  # Replace with the number of classes in your dataset\n",
        "\n",
        "# Create the CNN model\n",
        "model = create_cnn_model(input_shape, num_classes)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',  # Use 'sparse_categorical_crossentropy' if labels are integers\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "xPk6UMy-Q7ia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compile the Model:\n",
        "\n",
        "Compile your model with an appropriate loss function (e.g., categorical cross-entropy) and optimizer (e.g., Adam"
      ],
      "metadata": {
        "id": "2j3LbOuaRrDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',  # Use 'sparse_categorical_crossentropy' if labels are integers\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "bNVqoHdCRNhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**:\n",
        "\n",
        "Train your model using the training dataset. Use the validation dataset to monitor the model's performance during training. Here's an example of how to train a model in TensorFlow"
      ],
      "metadata": {
        "id": "1wPictTBSM8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the number of epochs and batch size for training\n",
        "epochs = 10  # Adjust the number of epochs as needed\n",
        "batch_size = 32  # Adjust the batch size as needed\n",
        "\n",
        "# Train the model using the training and validation datasets\n",
        "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate the model on the test dataset (if you have one)\n",
        "# test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "# print(\"Test Loss:\", test_loss)\n",
        "# print(\"Test Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "id": "OldPxbZySME3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation:\n",
        "\n",
        "Evaluate your trained model using appropriate metrics such as accuracy, precision, recall, and F1-score. Here's an example of how to calculate these metrics"
      ],
      "metadata": {
        "id": "CLTnlIjQSoQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_pred = model.predict(val_generator)\n",
        "y_true = val_generator.classes\n",
        "class_names = list(val_generator.class_indices.keys())\n",
        "\n",
        "report = classification_report(y_true, np.argmax(y_pred, axis=1), target_names=class_names)\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "-muwIrKBSi33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save the Model**:\n",
        "\n",
        "Once you are satisfied with your model's performance, save it for future use"
      ],
      "metadata": {
        "id": "EEvxuxYWLVnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('road_classification_model.h5')\n"
      ],
      "metadata": {
        "id": "EoncH-9eTMze"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}